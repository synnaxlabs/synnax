---
layout: "@/layouts/Reference.astro"
title: "Deployment Considerations"
description: "Essential concepts for deploying Synnax in production."
---

import { Divider, Note } from "@synnaxlabs/pluto";
import { Image } from "@/components/Media";
import { mdxOverrides } from "@/components/mdxOverrides";
import { Platform } from "@/components/platform";
export const components = mdxOverrides;

This page guides system administrators through the essential concepts they need to
deploy Synnax effectively in a production environment. This page will address the
topology of a Synnax deployment and the considerations that should be taken into account
when deploying Synnax (workload, networking, security, etc.). This page will also link
to more detailed guides on deployment workflows.

<Divider.Divider x />

## Key Terms

### Node

A node is a running instance of the Synnax database. It is a single executable that can
run on Windows, Linux, macOS, or inside of a container. A node's primary responsibility
is to permanently store and serve sensor data to users. It's useful to think of a node
as the heart of a Synnax deployment. All traffic related to data recording, control
sequences, and other tasks is routed through and processed by a node.

A node is not the same as the [Synnax Console](/reference/console/get-started). The
Console contains a local node embedded inside for single person usage of Synnax. For
deployment in a multi-user organization, it's common to run a node on a separate server
and have multiple users connect to it via the Console.

### Cluster

A cluster is a group of interconnected Cores that work together in order to function as
a single, unified database. Cores within a cluster will coordinate reads and writes to
distribute data storage and processing. This allows a cluster to scale horizontally,
growing to meet the needs of its application.

The smallest possible cluster is a single Core. The
[quick start](/reference/core/quick-start) guide is an example of a single-Core
deployment.

While single-Core clusters are less fault tolerant than multi-node deployments, it's
perfectly okay to deploy Synnax in production with just a single node. As your needs
grow, you can add additional nodes in order to balance the workload.

<Note.Note variant="info">
  Multi-Core clusters are experimental and not yet recommended for production use.
</Note.Note>

<Divider.Divider x />

## Deployment Considerations

### Workload

Perhaps the most important consideration when deploying Synnax is the workload that you
expect to place on your Core. Synnax is designed to handle very data intensive
operations, but performance is ultimately limited by the hardware specifications and
network quality.

The workloads that Synnax serves are generally split into two categories: real-time
hardware operations and long term data storage. We'll address each of these in turn.

#### Real-Time Hardware Operations

When using Synnax to control hardware, network stability and latency are the most
critical concerns. We recommend deploying Synnax on a local network with a dedicated
switch and high quality, well-protected cabling. Generally speaking, it's best to deploy
Synnax as close to the hardware as possible.

For single-Core clusters, we typically see our users deploy Synnax on the same computer
that their data acquisition devices are connected to. Here's an example of a simple
Synnax deployment. It has a central node that is connected to data acquisition devices
and operator machines.

<Image
  client:only="react"
  id="guides/sys-admin/deployment-considerations/real-time-deployment"
  themed={false}
/>

We caution against using a cloud deployment or any public network for real-time hardware
control, as this poses a significant risk to the stability of your system, and exposes
your Core to potential security threats.

#### Long Term Data Storage

When using Synnax for long term data storage or low-rate live streaming (less than 5
samples per second), network stability and latency are less critical, and the primary
concern becomes sustainable, scalable storage capacity and maximum availability to
users.

Our customers typically deploy Synnax on-prem within their own data centers, although
deploying Synnax in the cloud with a publicly exposed endpoint is also a viable option.
Synnax can run on bare metal, virtual machines, or you can use our docker images to
deploy Synnax in a containerized environment. Here's an example of a cloud deployment
with post processing systems, analyst machines, and a custom data application all
connecting to a single-Core cluster:

<Image
  client:only="react"
  id="guides/sys-admin/deployment-considerations/storage-deployment"
  themed={false}
/>

### Networking

Synnax uses two communication protocols: HTTP and gRPC. Understanding the details of
these protocols is not necessary for deploying Synnax. What's relevant is that they both
use standard TCP/IP networking.

All Synnax nodes expose themselves to traffic on a **single port**, which is
configurable at runtime. This port is used for cluster internal communication between
Cores, as well as for processing incoming requests from clients (Synnax Consoles,
control sequences, analysis scripts, Synnax Drivers, etc.).

When running in production, we recommend encrypting all traffic to and from your Synnax
cluster with SSL/TLS.

## Configuring NTP Clients

The following section assumes that a local NTP server has already been deployed.

<Platform.Tabs client:only="react" exclude={["Docker"]}>

<Fragment slot="Linux">

### Option 1: Chrony

Install chrony.

```bash
# For Debian/Ubuntu:
sudo apt update
sudo apt install chrony
```

```bash
# For RHEL/CentOS/Fedora:
sudo dnf install chrony
```

Edit the chrony configuration to point to your ntp server. Replace `<your-ntp-server>`
with the address of your local NTP server.

```bash
sudo nano /etc/chrony/chrony.conf
# Replace or add a line like:
# server <your-ntp-server-ip> iburst
```

Restart the chrony service to apply changes.

```bash
sudo systemctl restart chrony
```

Enable chrony to start on boot.

```bash
sudo systemctl enable chronyd.service
```

Start the chrony service.

```bash
sudo systemctl start chronyd.service
```

Check the synchronization status.

```bash
sudo systemctl status chronyd.service
```

<Divider.Divider x />

### Option 2: Timesyncd

This is the default NTP client in Ubuntu. While it will generally keep your time
synchronized, `chrony` will help with
[more complex configuration needs](https://documentation.ubuntu.com/server/explanation/networking/about-time-synchronisation/#about-timedatectl).

Ensure systemd-timesyncd is installed and enabled.

```bash
sudo systemctl enable systemd-timesyncd
sudo systemctl start systemd-timesyncd
```

Edit the configuration file to specify for NTP server(s). Replace `<your-ntp-server>`
with your NTP server address.

```bash
sudo nano /etc/systemd/timesyncd.conf
# Add or edit the line:
# NTP=<your-ntp-server-ip>
```

Restart the service to apply changes.

```bash
sudo systemctl restart systemd-timesyncd
```

Check synchronization status.

```bash
timedatectl status
```

</Fragment>

<Fragment slot="macOS">

### Using macOS

Check current NTP server(s).

```bash
sudo systemsetup -getnetworktimeserver
```

Set your preferred NTP server. Replace `<your-ntp-server-ip>` with your NTP server
address.

```bash
sudo systemsetup -setnetworktimeserver <your-ntp-server-ip>
```

Enable network time synchronization.

```bash
sudo systemsetup -setusingnetworktime on
```

Check synchronization status.

```bash
sudo sntp -sS <your-ntp-server-ip>
```

</Fragment>

<Fragment slot="Windows">

### Using Windows

Set your preferred NTP server (replace `<your-ntp-server-ip>` with your server address).

```powershell
w32tm /config /manualpeerlist:<your-ntp-server-ip>/syncfromflags:manual /reliable:yes /update
```

Restart the Windows Time service.

```powershell
net stop w32time
net start w32time
```

Force synchronization.

```powershell
w32tm /resync
```

Check synchronization status

```powershell
w32tm /query /status
```

</Fragment>

</Platform.Tabs>
