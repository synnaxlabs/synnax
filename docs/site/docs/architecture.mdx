---
sidebar_position: 4
---

# Architecture

Synnax's architecture is optimized for one objective: **reliable, efficient storage 
and transportation of telemetry across highly dynamic network topologies**. This page
is intended for developers who are interested in the inner workings of Synnax,
and is by no means required reading for those purely interested in using and 
operating the platform.

## Prerequisites

This page assumes the reader has an understanding of the high level concepts and
entities that make up the Synnax platform. We'll be using certain terms without
explaining them in detail, so we recommend you read the [Concepts](./concepts.mdx)
page before diving into this one.

## Overview

Synnax is a distributed system designed to operate as a cluster of nodes that
continually synchronize with one another to ensure reliable data storage and 
transmission. Synnax exposes its cluster as a monolithic data space, meaning
that a user can execute a query against any node and access the entire cluster's
data. 

This guide is broken into a series of sections that tell a story of not only
how Synnax works, but also why we made certain architectural decisions. If you're
interested in diving deeper into a particular topic, take a look at our [Technical
RFCs](../category/technical-rfcs) page, which contains a chronology of documents
that detail the design and implementation of Synnax's various subsystems.


## Design Principles

### Eventual Consistency

Most modern OLTP databases prioritize ACID compliance and strong consistency over
availability. This means that in the event of a network partition or multi-node
failure, the database would prefer to stop serving requests rather than risk
momentary data inconsistency. This is particularly important for financial systems, where 
the accumulation of inconsistencies can lead to hefty fines and customer distrust.

Hardware systems require a different set of tradeoffs. Consider a set
of sensors that detect over-pressures in a pipeline or gas tank. The failure of
a software system in detecting the overpressure could result in the loss of life
or valuable assets. An operator would prefer to use a system that continues to 
operate under volatile conditions and risk data inconsistency over one that
stops working entirely.

Synnax is optimized for the latter, and trades strong consistency for high 
availability and low latency. Its architecture is designed to ensure that
nodes can operate independently (albeit at reduced functionality) in the event
of complete network isolation.

### Data Regularity

Synnax makes another important tradeoff in the pursuit of performance: it requires
that all telemetry must be provided at fixed, regular intervals. This is ideal
for hardware and data acquisition scenarios, where data acquired from sensors
arrives at precise, fixed internals. Synnax's architecture makes several optimizations
surrounding this principle, especially in its on disk time-series engine.

Future plans include adding rudimentary support for irregularly sampled data, but
this is not a priority at the moment.

### Efficient Delivery

Synnax's architecture places extreme focus on efficient delivery of large volumes 
of data. While it is possible to use Synnax to store and query small amounts of
data, it performs exceptionally well when querying large ranges of telemetry.

Synnax's query APIs are also kept intentionally simple, and are inspired by
the [S3 API](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Operations.html).
Unlike most traditional databases, Synnax's users are performing extremely complex
analysis such as signal processing and simulation. These algorithms
are impossible to execute in a traditional query language like SQL. We'd rather
allow our users to leverage their existing tools and libraries over forcing them
to use our own, product specific query language.

## Software Architecture

Synnax uses a **domain oriented, layered architecture**. The separations between layers
represent clear software boundaries, meaning that each layer depends on the one below,
and is completely isolated from the ones above. Each layer also maintains a clear set
of domains, which are areas of responsibility targeted towards a specific piece of
functionality.

Synnax's architecture is composed of the following layers, from lowest to highest abstraction:

1. **Storage** - Reads and writes telemetry and cluster metadata to disk. 
2. **Distribution** - Aggregates storage from nodes into a single, monolithic data space. 
The distribution layer routes and executes queries from the service layer on the appropriate nodes.
3. **Service** - Contains services that define the core functionality of the system. These
services are largely network unaware, and focus on executing business logic.
4. **API** - Exposes a set of APIs available in various protocols. The API layer translates
requests into service calls, and aggregates responses from the service layer into cohesive results
for the user.

### Data Categories

Before diving into the details of each layer, it's important to understand the various
data categories that Synnax works with:

1. **Telemetry** - Sensor measurements most often represented as a binary encoded time-series.
2. **Metadata** - Information about the cluster, such as node versions, channel definitions,
users, permissions, etc. The amount of metadata a cluster stores is small in comparison
to the amount of telemetry it keeps on disk.
3. **State** - Monitoring and diagnostic information, such as node liveliness, channel write
states, metrics, etc.

Synnax handles and optimizes for each of these data categories very differently.

### Storage Layer

The storage layer is responsible for reading and writing cluster data to disk. Internally,
it is separated into two key domains: the telemetry and metadata engines. 

The telemetry engine, named [Cesium](../rfc/1-220517-cesium-segment-storage) receives read 
and write requests from the distribution layer and executes them as operations on disk.
The telemetry engine combines a key-value store with a custom binary data format to provide
efficient queries across large ranges of time.

The metadata engine, named [Aspen](../rfc/2-220518-aspen-distributed-storage) actually
spans both the storage and distribution layers. At the storage layer, it persists
cluster metadata to disk using a key-value store. We'll address Aspen's distribution.
layer responsibilities in the next section.

Our development team doesn't implement the key-value store used in Cesium or Aspen, but
rather relies on [Pebble](github.com/cockroachdb/pebble), a high performance key-value
store developed by the CockroachDB team.

### Distribution Layer

The distribution layer aggregates node local storage into a single, monolithic data space.
[Aspen](../rfc/2-220518-aspen-distributed-storage) is responsible for propagating cluster 
state and metadata to all nodes in the cluster. It's important to note that *each node 
has a complete copy of the cluster's metadata*. This is critical for maintaining 
high availability, as it allows nodes to continue serving read requests and some write 
requests even in the event of a failure. Aspen uses several gossip protocols to ensure
that all nodes are kept up to date.

Using propagated information about the cluster's topology, distribution layer services 
can route telemetry reads and writes to the appropriate nodes, where they are executed
on disk by the Storage layer. 

### Service Layer

The service layer contains the core algorithms that translate user provided API requests
into operations on the distribution layer. The service layer is, for the most part, network
unaware, and relies on the distribution layer to route requests to the appropriate nodes.

A few examples of services in this layer include:

- **Channel Service** - Responsible for creating, updating, and deleting channels. Channels
are the primary unit of organization in Synnax, and are used to group telemetry streams
into logical units. 
- **Segment Service** - Responsible for writing and reading chunks of telemetry, called
**segments** to and from disk.
- **User, Authentication, and Authorization Service** - Responsible for managing users,
permissions, and authentication tokens.


### API Layer

The uppermost layer of Synnax's architecture is the API layer. This layer is responsible
for exposing a set of APIs that users can use to interact with the system. Internally,
the API layer can be seprated into two sublayers. The first is a protocol agnostic layer,
which translates generic API requests into service calls. The second layer contains protocol
specific implementations of the API, such as HTTP, gRPC, WebSockets, etc.

## Further Reading

If you're interested in learning more about Synnax's architecture, the best place to
start is the [Cesium](../rfc/1-220517-cesium-segment-storage) and [Aspen](../rfc/2-220518-aspen-distributed-storage)
RFCs. These RFCs contain detailed descriptions of the telemetry and metadata engines,
and their respective distribution layer implementations. From there you can dive into 
the [Segment Distribution](../rfc/3-220604-segment-distribution) RFC, which describes 
how telemetry is distributed across nodes in the cluster. After that, feel free to browse
through any of the remaining RFCs, which describe the design of various services and APIs.