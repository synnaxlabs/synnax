# RFC 0027 - Oracle Migration System

## Status: Draft

## Summary

Design for a standardized migration system integrated with Oracle's schema-first code
generation. When Oracle schemas change, migrations are generated (automatically or via
prompt) in the Go layer, allowing developers to define custom migration logic. Current
type representations live in the parent package; legacy types and migration functions
live in a `migrations/` subdirectory.

## Goals

- Standardize migration tooling across all Oracle-managed types
- Strongly type server-side data structures (schematics, workspace layouts, etc.)
- Support automatic migration generation when Oracle schemas change
- Allow custom migration logic defined by the developer
- Enable eventual transition from msgpack to protobuf for storage encoding

## Design Decisions

### 1. Version Tracking Granularity

**Decision**: Per-type versioning with transitive dependency tracking.

Each Oracle type (e.g., `Schematic`, `Workspace`) has its own independent version
counter. Types can evolve at different rates. When type A depends on type B (e.g., a
struct field references another Oracle type), the migration system must track these
dependencies — a migration on type B may require a corresponding migration on type A.

### 2. Schema Change Detection

**Decision**: Manual migration generation with CI enforcement.

The developer manually runs a command (e.g., `oracle migrate generate`) to create
migration stubs when they change a schema. Oracle stores a snapshot of the schema state
at the time of the last migration generation. A CI check validates that the current
schema matches the last snapshot — if the schema has changed but no migration has been
generated, CI fails. This avoids over-eager migration generation while ensuring no
schema change ships without a corresponding migration.

### 3. Migration-Worthy Changes

**Decision**: All schema changes require a migration entry.

Any modification to an Oracle schema — adding a field, removing a field, renaming,
changing a field's type, changing required/optional status — requires a corresponding
migration. Explicit is better than implicit. Even additive changes that are technically
safe with msgpack deserialization get a migration entry. This ensures a complete audit
trail and prevents surprises when the storage codec changes (e.g., msgpack → protobuf
where additive changes may not be free).

### 4. Migration Function Signature

**Decision**: Typed per-entry transform as default, with raw transaction escape hatch.

The primary API is a typed per-entry transform:

```go
func(ctx context.Context, old OldSchematic) (NewSchematic, error)
```

The framework handles iteration over all entries of that type, decoding with the old
type, calling the transform, and writing back the new type. For complex cases (cross-type
migrations, ontology restructuring), a raw transaction fallback is available:

```go
func(ctx context.Context, tx gorp.Tx) error
```

The typed approach is preferred and should cover the vast majority of cases. The raw
transaction mode exists as an escape hatch, not the default path. We need to enumerate
concrete migration scenarios to validate that the typed API covers them without needing
the escape hatch too often.

### 5. Migration Execution Strategy

**Decision**: Eager migration on server startup.

All entries of a type are migrated during server startup before the service accepts
requests. This is consistent with how `EntryManager` already works and guarantees that
all data is in the current format at runtime — no need to support reading both old and
new formats simultaneously. The tradeoff is potentially slower startup with large
datasets, but metadata entries (schematics, workspaces, devices) are expected to number
in the thousands, not millions, so this is acceptable.

### 6. Upgrade Path and Rollback

**Decision**: Sequential chaining, strictly forward-only, no rollback.

Migrations run in sequential order (v1→v2→v3). Each migration only handles a single
version step. When a customer skips versions (e.g., v0.48 → v0.51), all intermediate
migrations execute in order. This matches the existing `gorp.Migrator` pattern.

No rollback support. Migrations are tested before shipping to customers — development
data is disposable, so broken migrations during development are fixed by wiping local
data and re-running. If a bad migration ships to customers, a corrective migration is
released in a subsequent version.

### 7. Directory Layout

**Decision**: Per-service `migrations/` directory.

Legacy types and migration functions live co-located with the service that owns the type:

```
core/pkg/service/schematic/
├── schematic.go           # Current type (generated by Oracle)
├── types.gen.go           # Generated code
├── service.go             # Service logic
└── migrations/
    ├── v1.go              # Legacy Schematic v1 type + v1→v2 migration
    ├── v2.go              # Legacy Schematic v2 type + v2→v3 migration
    └── migrate.go         # Migration registration / wiring
```

The current (latest) type representation lives in the parent package. The `migrations/`
subdirectory contains only legacy types and the transform functions to migrate them
forward. This keeps migration logic close to the code that uses the type.

### 8. Legacy Type Generation

**Decision**: Auto-generated snapshots by Oracle.

When the developer runs `oracle migrate generate`, Oracle snapshots the current type
definition into the `migrations/` directory as a Go struct before applying the schema
change. The developer then writes the transform function that maps the old type to the
new type. This eliminates boilerplate and ensures the legacy type exactly matches what
was previously stored. Oracle maintains an internal record of the schema state at each
migration point to enable accurate snapshot generation.

### 9. Protobuf Transition Scope

**Decision**: Replace msgpack entirely with protobuf for all gorp entries.

The goal is a full codec replacement — all Oracle-managed types stored via gorp switch
from msgpack to protobuf encoding. The migration system handles the format transition
(decode old entries with msgpack, re-encode with protobuf). If a full cutover proves
impractical, a per-type gradual migration is acceptable, but the strong preference is a
clean, complete switch. Oracle already generates `.proto` files and Go translators, so
the protobuf infrastructure exists.

### 10. Codec Migration (msgpack → protobuf)

**Decision**: Oracle-generated codec migration.

Since Oracle owns the schema and generates both msgpack-compatible Go structs and
protobuf definitions + translators, the codec transition should be Oracle-generated
rather than manually written per-type. The exact mechanism (gorp-layer detection vs.
explicit migration entry) is an implementation detail to be resolved during development.
The key principle is that Oracle should be able to generate all the code needed for the
codec switch — the developer shouldn't have to write repetitive msgpack→protobuf
boilerplate for each type.

### 11. Strongly Typing JSON Fields

**Decision**: Full struct definitions, adopted incrementally.

The end goal is to define the complete nested structure of complex fields (schematic
data, workspace layout, etc.) as Oracle types — symbols, connections, positions, the
entire tree. Oracle generates Go/TS/Python types for maximum type safety.

The adoption path is incremental: start by defining the top-level structure as an Oracle
type while nested fields remain `json`/`any`. Then use the migration system itself to
progressively promote nested fields to strongly typed Oracle types. Each promotion step
is a schema change + migration, eating the elephant one bite at a time.

### 12. Client-Server Type Source of Truth

**Decision**: Gradual replacement with Oracle as the eventual source of truth.

Oracle-generated types will coexist with manually defined TypeScript types during the
transition. Over time, manual client-side definitions are migrated to Oracle schemas.
Some client-only types (UI state, local caching) may always remain manual. The migration
system supports this by allowing the server to handle schema evolution independently of
the client's adoption timeline.

### 13. Multi-Node Migration Coordination

**Decision**: Each node migrates its own local KV data independently.

Each node in a Synnax cluster runs migrations on its own local KV store during startup.
There is no leader election or distributed locking for migrations. This works because
Aspen gossip replicates metadata — each node holds a local copy that it can migrate
independently. The implication is that migrations must be deterministic and idempotent:
given the same input data, every node must produce the same output.

### 14. Schema Snapshot Format

**Decision**: TBD — requires trade study.

Oracle needs a stored representation of the schema at each migration point for two
purposes: (1) CI change detection (diff current schema vs. last snapshot), and (2)
auto-generating legacy types for migration functions. Options under consideration:

- **Committed `.oracle` snapshots**: Copy `.oracle` files into a versioned snapshot
  directory. Human-readable, git-diffable, can regenerate legacy types from source.
- **Structured AST (JSON/binary)**: Serialize parsed schema into machine-readable
  format. Compact, programmatically diffable, but not human-readable.
- **Hash + generated Go code**: Store a lightweight hash for detection; legacy types
  committed as generated Go structs. Hash is fast to check; Go code is the real record.

Trade study needed to evaluate which approach best balances developer experience, CI
reliability, and legacy type generation accuracy.

### 15. Protobuf Field Number Stability

**Decision**: Not a concern — excluded from migration system scope.

Protobuf field number stability does not matter because clients are guaranteed to be in
sync with protobuf versions. Since migrations run eagerly on startup and convert all
stored data to the current schema, there is never a scenario where old
protobuf-encoded data is read with new field numbers. Oracle can freely assign and
reassign field numbers on each generation without stability constraints.

### 16. Version Counter Storage

**Decision**: Single KV key per type.

Each Oracle-managed type gets a dedicated KV key (e.g., `__oracle__/schematic/version`)
that stores its current migration version. All entries of that type are assumed to be at
the same version after startup migration completes. This is consistent with the eager
migration model — once startup finishes, every entry has been migrated. No version field
is added to individual entries.

### 17. Version Counter Size

**Decision**: uint16 (max 65,535 migrations per type).

Upgrade from the current uint8 (255) to uint16 to future-proof against the cumulative
effect of requiring migrations for all schema changes. 65,535 is more than sufficient
while remaining cheap to store (2 bytes in the KV store).

### 18. Integration with Existing Infrastructure

**Decision**: Oracle migrations replace `gorp.Migrator` entirely.

The Oracle migration system supersedes `gorp.Migrator` as the single migration
mechanism. Existing migrations (ranger range groups, channel name validation, etc.) will
be ported to the Oracle migration system. `gorp.EntryManager`'s automatic key-encoding
migration may remain as a low-level concern, but all business-logic and schema-evolution
migrations go through Oracle. One unified system.

### 19. Migration Stub Generation

**Decision**: Always generate an empty skeleton.

When Oracle generates a migration, it creates the legacy type snapshot and an empty
transform function with the correct signatures. The developer writes all migration logic
manually. No auto-generated defaults — this avoids the risk of incorrect automatic
transforms silently corrupting data. The developer must explicitly handle every field
mapping, even for simple additions.

### 20. Non-Oracle Type Migration

**Decision**: TBD — requires trade study.

Some gorp-stored types (Cesium internals, Aspen KV metadata, ontology resources) are not
currently Oracle-managed. Options:

- **Convert everything to Oracle**: All gorp-stored types become `.oracle` schemas. Full
  consistency but requires porting effort.
- **Support both Oracle and manual types**: The migration system accepts both
  Oracle-generated and hand-written type definitions. Pragmatic for gradual adoption.
- **Separate concerns**: Oracle migrations handle Oracle types only. Non-Oracle types
  retain their existing mechanisms (`x/go/migrate`, custom Cesium versioning). Clean
  boundaries but two parallel systems.

Trade study needed to evaluate porting effort vs. maintenance cost of dual systems.

### 21. MVP / First Use Case

**Decision**: msgpack → protobuf codec transition.

The first migration to run through the Oracle migration system is the codec switch from
msgpack to protobuf for gorp-stored entries. This is a high-value change that exercises
the core migration pipeline (iterate all entries, decode with old codec, re-encode with
new codec) without requiring schema changes to any individual type. It validates the
infrastructure before more complex per-type schema migrations are needed.

**MVP requirements for this use case**:

- Migration execution on server startup (per-type, eager)
- Per-type version tracking (KV key, uint16)
- Oracle-generated migration code (decode msgpack → encode protobuf)
- Independent per-node execution
- CI check that migrations are generated when needed

### 22. Migration Testing

**Decision**: Built-in test helpers provided by the framework.

The migration framework provides test utilities that reduce boilerplate for migration
authors. Helpers like `TestMigration(oldEntries, expectedNewEntries)` handle:

- Setting up an in-memory KV store populated with old-format data
- Running the migration function
- Asserting that results match expected new-format entries
- Validating that the version counter was incremented correctly

This ensures consistent testing patterns across all migrations and makes it easy for
developers to write thorough migration tests without reimplementing setup/teardown logic
each time.

### 23. Relationship to RFC 0025

**Decision**: This is the foundation that RFC 0025 depends on.

The Oracle migration system builds the server-side infrastructure for schema evolution.
RFC 0025 (moving client-side migrations to the server) will layer on top once server
types are strongly typed via Oracle. The sequence is:

1. **This RFC (0027)**: Oracle migration system + msgpack→protobuf codec transition
2. **Strongly type JSON fields**: Incrementally promote `json` fields to Oracle types
3. **RFC 0025**: Server owns all schema evolution; client sends/receives Oracle-typed
   data instead of managing its own migrations

## Migration Scenarios

This section walks through every known migration pattern — real and anticipated — with
concrete code showing what the migration file, legacy type, and transform function would
look like under the proposed system. The purpose is to stress-test the API design before
building it.

### Scenario 1: Codec Transition (msgpack → protobuf)

**Context**: The MVP. Every Oracle-managed type switches storage encoding. No schema
change — same fields, same types, different wire format.

**What Oracle generates** (`core/pkg/service/schematic/migrations/v1.go`):

```go
package migrations

import (
    "context"

    "github.com/google/uuid"
    "github.com/synnaxlabs/x/go/binary"
)

// SchematicV1 is the legacy type snapshot. Same fields, msgpack encoding.
type SchematicV1 struct {
    Key      uuid.UUID                 `json:"key" msgpack:"key"`
    Name     string                    `json:"name" msgpack:"name"`
    Data     binary.MsgpackEncodedJSON `json:"data" msgpack:"data"`
    Snapshot bool                      `json:"snapshot" msgpack:"snapshot"`
}

func (s SchematicV1) GorpKey() uuid.UUID { return s.Key }
func (s SchematicV1) SetOptions() []any  { return nil }
```

**What the developer writes** (transform function):

```go
func MigrateV1ToV2(ctx context.Context, old SchematicV1) (schematic.Schematic, error) {
    return schematic.Schematic{
        Key:      old.Key,
        Name:     old.Name,
        Data:     old.Data,
        Snapshot: old.Snapshot,
    }, nil
}
```

**Observations**:

- The transform itself is trivial — just field copying. The real work is that the
  migration runner decodes with the msgpack codec and writes back with the protobuf
  codec.
- This is identical for every type. Oracle generates the full migration (transform
  included) since it's purely mechanical. The developer writes nothing — Oracle's "full
  mode" generation handles codec transitions end-to-end.
- Every Oracle-managed type (~15+ types) gets a generated migration file. Repetitive but
  explicit, auditable, and consistent with "all changes require migration."

---

### Scenario 2: Add a Field

**Context**: Add a `description` field to `Workspace`. Additive change — old entries
don't have it, new entries do.

**Schema change** (`schemas/workspace.oracle`):

```diff
 Workspace struct {
     key    Key    { @key }
     name   string { @validate required }
     author uuid?
     layout json   { @ts preserve_case }
+    description string?
 }
```

**Oracle-generated legacy type** (`core/pkg/service/workspace/migrations/v2.go`):

```go
package migrations

// WorkspaceV2 is the schema at version 2 (before description was added).
type WorkspaceV2 struct {
    Key    uuid.UUID                 `json:"key" msgpack:"key"`
    Name   string                    `json:"name" msgpack:"name"`
    Author uuid.UUID                 `json:"author" msgpack:"author"`
    Layout binary.MsgpackEncodedJSON `json:"layout" msgpack:"layout"`
}
```

**Developer-written transform**:

```go
func MigrateV2ToV3(
    ctx context.Context,
    old WorkspaceV2,
) (workspace.Workspace, error) {
    return workspace.Workspace{
        Key:         old.Key,
        Name:        old.Name,
        Author:      old.Author,
        Layout:      old.Layout,
        Description: nil, // new field, no existing data
    }, nil
}
```

**Observations**:

- With msgpack, this migration is technically unnecessary — missing fields decode as
  zero values. But we decided all changes require a migration (Decision 3), so it exists
  for the audit trail and for protobuf compatibility.
- The typed per-entry API handles this cleanly. No escape hatch needed.

---

### Scenario 3: Remove a Field

**Context**: Remove the `author` field from `Workspace` (hypothetical).

**Schema change**:

```diff
 Workspace struct {
     key    Key    { @key }
     name   string { @validate required }
-    author uuid?
     layout json   { @ts preserve_case }
 }
```

**Oracle-generated legacy type** (`core/pkg/service/workspace/migrations/v3.go`):

```go
type WorkspaceV3 struct {
    Key    uuid.UUID                 `json:"key" msgpack:"key"`
    Name   string                    `json:"name" msgpack:"name"`
    Author uuid.UUID                 `json:"author" msgpack:"author"`
    Layout binary.MsgpackEncodedJSON `json:"layout" msgpack:"layout"`
}
```

**Developer-written transform**:

```go
func MigrateV3ToV4(
    ctx context.Context,
    old WorkspaceV3,
) (workspace.Workspace, error) {
    return workspace.Workspace{
        Key:    old.Key,
        Name:   old.Name,
        Layout: old.Layout,
        // Author deliberately dropped
    }, nil
}
```

**Observations**:

- Clean — typed API handles it naturally. The old type has the field, the new type
  doesn't. The developer explicitly chooses not to copy it.
- With protobuf, this also reclaims storage (the field is no longer serialized).

---

### Scenario 4: Rename a Field

**Context**: Rename `data` to `spec` in `Schematic` (hypothetical).

**Schema change**:

```diff
 Schematic struct {
     key      Key    { @key }
     name     string
-    data     json   { @ts preserve_case }
+    spec     json   { @ts preserve_case }
     snapshot bool
 }
```

**Oracle-generated legacy type**:

```go
type SchematicV3 struct {
    Key      uuid.UUID                 `json:"key" msgpack:"key"`
    Name     string                    `json:"name" msgpack:"name"`
    Data     binary.MsgpackEncodedJSON `json:"data" msgpack:"data"`
    Snapshot bool                      `json:"snapshot" msgpack:"snapshot"`
}
```

**Developer-written transform**:

```go
func MigrateV3ToV4(
    ctx context.Context,
    old SchematicV3,
) (schematic.Schematic, error) {
    return schematic.Schematic{
        Key:      old.Key,
        Name:     old.Name,
        Spec:     old.Data, // renamed: data → spec
        Snapshot: old.Snapshot,
    }, nil
}
```

**Observations**:

- Typed API handles this perfectly — the developer maps old field to new field name.
- This is where manual transforms shine over auto-generation: the system can't know
  that `data` became `spec` without developer input.

---

### Scenario 5: Strongly Type a JSON Field (Incremental)

**Context**: Promote `Schematic.data` from `json` to a top-level struct, but keep
nested fields as `json` initially.

**Schema change**:

```diff
+SchematicData struct {
+    symbols json { @ts preserve_case }
+    connections json { @ts preserve_case }
+    viewport json { @ts preserve_case }
+}

 Schematic struct {
     key      Key    { @key }
     name     string
-    data     json   { @ts preserve_case }
+    data     SchematicData
     snapshot bool
 }
```

**Oracle-generated legacy type**:

```go
type SchematicV4 struct {
    Key      uuid.UUID                 `json:"key" msgpack:"key"`
    Name     string                    `json:"name" msgpack:"name"`
    Data     binary.MsgpackEncodedJSON `json:"data" msgpack:"data"`
    Snapshot bool                      `json:"snapshot" msgpack:"snapshot"`
}
```

**Developer-written transform**:

```go
func MigrateV4ToV5(
    ctx context.Context,
    old SchematicV4,
) (schematic.Schematic, error) {
    // Parse the unstructured JSON into the new structured type.
    // Since the nested fields are still `json`, this is mainly
    // pulling out top-level keys.
    rawData := old.Data.Map()
    return schematic.Schematic{
        Key:  old.Key,
        Name: old.Name,
        Data: schematic.SchematicData{
            Symbols:     rawData["symbols"],
            Connections: rawData["connections"],
            Viewport:    rawData["viewport"],
        },
        Snapshot: old.Snapshot,
    }, nil
}
```

**Observations**:

- This is the first scenario where the transform does real work — parsing unstructured
  JSON into a structured type.
- The typed per-entry API handles it well. The developer extracts known keys from the
  JSON blob and maps them to struct fields.
- **Key question**: What happens if a stored entry has unexpected keys in the JSON blob
  (e.g., client wrote extra fields the server didn't know about)? The migration must
  decide: drop them, preserve them in a catch-all field, or error? This needs a design
  decision for the strongly-typing migration path.

---

### Scenario 6: Change a Field's Type

**Context**: Change `Workspace.author` from `uuid?` to a new `Author` struct
(hypothetical).

**Schema change**:

```diff
+Author struct {
+    key  uuid
+    name string
+}

 Workspace struct {
     key    Key    { @key }
     name   string { @validate required }
-    author uuid?
+    author Author?
     layout json   { @ts preserve_case }
 }
```

**Developer-written transform**:

```go
func MigrateV5ToV6(
    ctx context.Context,
    old WorkspaceV5,
) (workspace.Workspace, error) {
    var author *workspace.Author
    if old.Author != uuid.Nil {
        // Need to look up the user name — but we only have the entry, not a
        // database handle. This requires cross-type data access.
        author = &workspace.Author{
            Key:  old.Author,
            Name: "", // Can't resolve without DB access!
        }
    }
    return workspace.Workspace{
        Key:    old.Key,
        Name:   old.Name,
        Author: author,
        Layout: old.Layout,
    }, nil
}
```

**Observations**:

- **This breaks the typed per-entry API.** The transform needs to look up a `User` by
  UUID to populate `Author.Name`, but the typed signature
  `func(ctx, old) (new, error)` doesn't provide database access.
- **Options**:
  1. Accept incomplete data (set `Name: ""`) and backfill later.
  2. Pass additional context (like a user lookup function) via the context parameter.
  3. Use the raw transaction escape hatch for this migration.
- This is a real case where the escape hatch is needed, or the typed API needs to be
  enriched with injectable dependencies.

---

### Scenario 7: Port Existing Ranger Migration (Cross-Type Structural)

**Context**: The existing `migrateRangeGroups` migration restructures ontology
relationships — it reads groups, queries children, creates new parent ranges, and
rewires ontology edges. This is a cross-type, cross-service migration.

**Under the new system** (`core/pkg/service/ranger/migrations/v1.go`):

```go
func MigrateV1RangeGroups(ctx context.Context, tx gorp.Tx) error {
    // This is the raw transaction escape hatch — the typed per-entry API
    // cannot express this migration because it:
    // 1. Reads from multiple types (Group, Range, ontology.Resource)
    // 2. Creates new entries (Range)
    // 3. Modifies ontology relationships
    // 4. Deletes groups

    // ... existing migrateRangeGroups logic, unchanged ...
}
```

**Observations**:

- This migration fundamentally cannot use the typed per-entry API. It operates across
  types, creates new entries, and modifies external systems (ontology).
- The raw transaction escape hatch is essential for this case.
- **Question**: How does this migration get access to the ontology writer and group
  service? The current code accesses them via `s.cfg.Ontology` and `s.cfg.Group` on the
  service struct. Under the new system, do migrations receive a dependency injection
  context?

---

### Scenario 8: Port Existing RBAC Migration (Cross-Type + External State)

**Context**: The RBAC migration reads legacy policies, determines user roles based on
policy analysis, assigns roles, and deletes legacy policies. It accesses the user
service, role service, and ontology.

**Under the new system**:

```go
func MigrateV1RBAC(ctx context.Context, tx gorp.Tx) error {
    // Raw transaction escape hatch.
    // Needs access to: User service, Role service, Ontology, provisioned role keys.
    // This migration can't be typed per-entry because:
    // 1. It reads LegacyPolicy entries
    // 2. It reads User entries
    // 3. It writes Role assignments
    // 4. It deletes LegacyPolicy entries
    // 5. It depends on external state (provisioned role keys)

    // ... existing MigratePermissions logic ...
}
```

**Observations**:

- Same as Scenario 7: raw transaction escape hatch is required.
- **Additional concern**: This migration depends on external state (provisioned role
  keys from `ProvisionResult`). How are external dependencies injected into migration
  functions? The migration function signature needs access to more than just `gorp.Tx`.

---

### Scenario 9: Transitive Dependency — Type B Changes, Type A References B

**Context**: `Device` has a `rack` field of type `rack.Key`. If `rack.Key` changes from
`uint32` to `uint64`, Device also needs a migration even though its schema didn't change
directly.

**Rack schema change**:

```diff
-Key = uint32
+Key = uint64
```

**Device migration needed** (`core/pkg/service/device/migrations/v2.go`):

```go
type DeviceV2 struct {
    Key        string                    `json:"key" msgpack:"key"`
    Rack       uint32                    `json:"rack" msgpack:"rack"` // old type
    Location   string                    `json:"location" msgpack:"location"`
    // ... other fields
}

func MigrateV2ToV3(
    ctx context.Context,
    old DeviceV2,
) (device.Device, error) {
    return device.Device{
        Key:      old.Key,
        Rack:     rack.Key(uint64(old.Rack)), // widen uint32 → uint64
        Location: old.Location,
        // ...
    }, nil
}
```

**Observations**:

- Oracle's transitive dependency tracking (Decision 1) must detect that `Device`
  references `rack.Key` and flag that a Device migration is needed when `rack.Key`
  changes.
- The CI check should fail if `rack.Key` changes but Device doesn't have a
  corresponding migration.
- The typed per-entry API handles the actual transform fine.

---

## Scenario Analysis Summary

| Scenario | Typed Per-Entry | Raw Tx Needed | Oracle Can Auto-Generate | Dependencies |
|----------|----------------|---------------|--------------------------|--------------|
| 1. Codec transition | Yes (trivial) | No | Yes (mechanical) | None |
| 2. Add field | Yes | No | Possible (zero value) | None |
| 3. Remove field | Yes | No | Possible (drop) | None |
| 4. Rename field | Yes | No | No (ambiguous) | None |
| 5. Strongly type JSON | Yes | No | No (parsing logic) | None |
| 6. Change field type | **Partial** | **Maybe** | No | Cross-type lookup |
| 7. Ranger groups | No | **Yes** | No | Ontology, Group svc |
| 8. RBAC permissions | No | **Yes** | No | User, Role, Ontology |
| 9. Transitive dep | Yes | No | Possible (widening) | Dep tracking |

### Key Findings

**The typed per-entry API covers 6 of 9 scenarios cleanly.** Scenarios 7 and 8 require
the raw transaction escape hatch. Scenario 6 is borderline — it works if you accept
incomplete data or enrich the API.

**Oracle has two generation modes.** Mechanical migrations (codec transitions, trivial
field widening) are fully auto-generated — no developer input. Schema-change migrations
(field add/remove/rename, type changes, JSON strong-typing) generate a skeleton for the
developer to fill in. This minimizes user-written code while keeping the developer in
control where judgment is needed.

**Cross-type migrations get enriched dependency injection.** Scenarios 7 and 8 need
access to services (ontology, group, user, role). The raw transaction escape hatch will
be enriched to inject service dependencies beyond `gorp.Tx`.

**Transitive dependencies (Scenario 9) need Oracle support.** When type B changes and
type A references B, Oracle must detect this and require a migration on A. This is
schema-level analysis, not runtime.

**Unknown JSON keys during strong-typing (Scenario 5) is unresolved.** When promoting
`json` to a struct, what happens to unexpected keys? Needs a design decision.

## Mechanical Analysis: How Migrations Actually Execute

This section traces through the real gorp code to understand what the migration runner
must do at the KV level. The scenarios above showed the *what* (transform functions);
this section shows the *how* (the framework machinery that calls them).

### The Key Prefix Problem

Gorp's `Reader[K, E]` and `Writer[K, E]` derive their KV prefix from the Go type name
via `types.Name[E]()`. For example:

- `Reader[uuid.UUID, Schematic]` → prefix `__gorp__//Schematic`
- `Reader[uuid.UUID, SchematicV1]` → prefix `__gorp__//SchematicV1`

**Problem**: Data stored as `Schematic` lives under prefix `__gorp__//Schematic`. If the
migration runner tries to read it using `Reader[uuid.UUID, SchematicV1]`, it gets prefix
`__gorp__//SchematicV1` — no entries found.

**Implication**: The migration runner **cannot use gorp's typed Reader/Writer** with
legacy types directly. It must operate at a lower level.

### How EntryManager Already Solves This

`manager.go` already has a migration that reads and rewrites entries. Here's what
`reEncodeKeys` does:

```go
func reEncodeKeys[K Key, E Entry[K]](ctx context.Context, tx Tx) error {
    iter, err := WrapReader[K, E](tx).OpenIterator(IterOptions{})  // uses current type's prefix
    // ...
    writer := WrapWriter[K, E](tx)
    for iter.First(); iter.Valid(); iter.Next() {
        writer.BaseWriter.Delete(ctx, iter.Key())     // delete old KV entry
        writer.Set(ctx, *iter.Value(ctx))              // write new KV entry
    }
}
```

This works because it reads and writes the **same type** — it's re-encoding keys, not
changing the type shape. The codec (msgpack) decodes into `E` and re-encodes `E`.

### What the Migration Runner Must Do Differently

For a schema migration (e.g., `SchematicV1` → `Schematic`), the runner needs to:

1. **Iterate raw KV entries** under the canonical prefix (`__gorp__//Schematic`)
2. **Decode raw bytes** using the old codec into the old type (`SchematicV1`)
3. **Call the transform** function: `old SchematicV1 → new Schematic`
4. **Encode the new type** using the current codec
5. **Write back** under the same KV key

Step 2 is the critical one. The runner can't use `WrapReader[K, SchematicV1]` (wrong
prefix). Instead, it must:

```go
// Open a raw KV iterator with the canonical prefix
prefix := []byte("__gorp__//Schematic")
rawIter, _ := tx.OpenIterator(kv.IterPrefix(prefix))

// For each entry:
for rawIter.First(); rawIter.Valid(); rawIter.Next() {
    rawKey := rawIter.Key()
    rawValue := rawIter.Value()

    // Decode raw bytes into old type using the old codec
    var old SchematicV1
    oldCodec.Decode(ctx, rawValue, &old)

    // Call the developer's transform function
    new, _ := transformFn(ctx, old)

    // Encode new type using the current codec
    newValue, _ := newCodec.Encode(ctx, new)

    // Delete old entry and write new one (key stays the same)
    tx.Delete(ctx, rawKey)
    tx.Set(ctx, rawKey, newValue)
}
```

### Multi-Step Chaining (v1 → v2 → v3)

When a customer jumps from v1 to v3, the runner chains transforms in memory:

```
Raw bytes from KV → decode as V1 → transform to V2 → transform to V3 → encode → write
```

Intermediate types (V2) are never written to disk. The runner decodes once from KV,
chains all transforms, encodes once, and writes once. This requires that each migration
step's output type matches the next step's input type.

### Codec Transition Specifics

For the msgpack → protobuf MVP, the runner needs two codecs simultaneously:

```go
oldCodec := &binary.MsgPackCodec{}     // for reading existing data
newCodec := &binary.ProtobufCodec{}    // for writing new data
```

The transform function is identity (same fields), so the chain is:

```
Raw msgpack bytes → decode as Schematic → (identity transform) → encode as protobuf → write
```

The canonical type prefix (`__gorp__//Schematic`) doesn't change — only the value
encoding does.

### The Canonical Prefix Registry

The migration runner needs to know the canonical KV prefix for each type — the prefix
under which data is actually stored. This can't be derived from the legacy type name.
Options:

1. **Oracle generates the prefix**: Each migration file includes the canonical prefix as
   a constant. Oracle knows the type name from the schema.
2. **Registry at registration time**: When migrations are registered, the current type
   `E` is used to derive the prefix via `types.Name[E]()`, and that prefix is passed to
   the runner.
3. **Convention**: The prefix is always `__gorp__//<OracleTypeName>`, matching the
   Oracle schema's Go type name. The runner computes it from the schema.

Option 2 is most robust — the migration runner is parameterized on the *current* type
(which exists at compile time) and derives the prefix from it.

## Go API Surface

Based on the mechanical analysis, here are the concrete interfaces and types.

### Core Types

```go
package oracle

import (
    "context"
    "encoding/binary"

    xbinary "github.com/synnaxlabs/x/binary"
    "github.com/synnaxlabs/x/errors"
    "github.com/synnaxlabs/x/gorp"
    "github.com/synnaxlabs/x/kv"
    "github.com/synnaxlabs/x/query"
)

// TypedMigration is a per-entry transform function. The migration runner handles
// iteration, decoding, and encoding. The developer only writes the field mapping.
//
// I is the old (input) type, O is the new (output) type. Both must be
// serializable by the configured codecs.
type TypedMigration[I, O any] func(ctx context.Context, old I) (O, error)

// RawMigration is the escape hatch for cross-type or complex migrations that
// can't be expressed as a per-entry transform. The developer gets full
// transaction access and is responsible for all reads/writes.
type RawMigration func(ctx context.Context, tx gorp.Tx) error

// Migration is a single versioned migration step. Exactly one of Typed or Raw
// must be set.
type Migration struct {
    // Name is a human-readable identifier for this migration (e.g.,
    // "add_description_field", "msgpack_to_protobuf").
    Name string

    // migrate is the internal function that the runner calls. Set by the
    // registration helpers (RegisterTyped, RegisterRaw), not by the user
    // directly.
    migrate func(ctx context.Context, tx kv.Tx, cfg runConfig) error
}
```

### Registration Helpers

```go
// RegisterTyped creates a Migration from a typed per-entry transform. The
// runner will:
//   1. Open a raw KV iterator under the canonical prefix for O
//   2. Decode each value using oldCodec into type I
//   3. Call the transform function
//   4. Encode the result using newCodec as type O
//   5. Write back under the same key
//
// The oldCodec / newCodec distinction handles codec transitions. For
// schema-only changes (no codec switch), both codecs are the same.
func RegisterTyped[I, O any](
    name string,
    transform TypedMigration[I, O],
    opts ...MigrationOption,
) Migration {
    return Migration{
        Name: name,
        migrate: func(ctx context.Context, tx kv.Tx, cfg runConfig) error {
            oldCodec := cfg.oldCodec
            newCodec := cfg.newCodec
            iter, err := tx.OpenIterator(kv.IterPrefix(cfg.prefix))
            if err != nil {
                return err
            }
            defer iter.Close()

            for iter.First(); iter.Valid(); iter.Next() {
                var old I
                if err := oldCodec.Decode(ctx, iter.Value(), &old); err != nil {
                    return errors.Wrapf(err, "migration %s: decode failed", name)
                }
                new, err := transform(ctx, old)
                if err != nil {
                    return errors.Wrapf(err, "migration %s: transform failed", name)
                }
                encoded, err := newCodec.Encode(ctx, new)
                if err != nil {
                    return errors.Wrapf(err, "migration %s: encode failed", name)
                }
                if err := tx.Set(ctx, iter.Key(), encoded); err != nil {
                    return err
                }
            }
            return iter.Error()
        },
    }
}

// RegisterRaw creates a Migration from a raw transaction function. Used for
// cross-type migrations that need full DB access.
func RegisterRaw(name string, fn RawMigration) Migration {
    return Migration{
        Name: name,
        migrate: func(ctx context.Context, tx kv.Tx, cfg runConfig) error {
            return fn(ctx, gorp.WrapTx(tx, cfg.newCodec))
        },
    }
}
```

### Migration Runner

```go
// Migrator runs migrations for a single Oracle-managed type. It tracks the
// current version in the KV store and executes pending migrations sequentially.
type Migrator[K gorp.Key, E gorp.Entry[K]] struct {
    // Migrations is the ordered list of migrations for this type.
    Migrations []Migration
}

type runConfig struct {
    prefix   []byte
    oldCodec xbinary.Codec
    newCodec xbinary.Codec
}

// Run executes all pending migrations for this type. Called during service
// startup, before the service accepts requests.
func (m Migrator[K, E]) Run(ctx context.Context, db *gorp.DB) error {
    // Derive the canonical prefix from the current type E.
    prefix := []byte("__gorp__//" + types.Name[E]())

    // Version key in KV store.
    versionKey := []byte("__oracle__/" + types.Name[E]() + "/version")

    return db.WithTx(ctx, func(tx gorp.Tx) error {
        // Read current version (uint16, big-endian).
        currentVersion, err := readVersion(ctx, tx, versionKey)
        if err != nil {
            return err
        }

        if int(currentVersion) >= len(m.Migrations) {
            return nil // already up to date
        }

        for i := currentVersion; i < uint16(len(m.Migrations)); i++ {
            migration := m.Migrations[i]

            cfg := runConfig{
                prefix:   prefix,
                oldCodec: db.Codec,  // current DB codec (decode existing data)
                newCodec: db.Codec,  // same codec by default
            }

            if err := migration.migrate(ctx, tx.KVTx(), cfg); err != nil {
                return errors.Wrapf(err, "migration %d (%s) failed",
                    i+1, migration.Name)
            }

            // Increment version.
            if err := writeVersion(ctx, tx, versionKey, i+1); err != nil {
                return err
            }
        }
        return nil
    })
}

func readVersion(ctx context.Context, tx gorp.Tx, key []byte) (uint16, error) {
    b, closer, err := tx.Get(ctx, key)
    if errors.Is(err, query.ErrNotFound) {
        return 0, nil
    }
    if err != nil {
        return 0, err
    }
    defer closer.Close()
    if len(b) < 2 {
        return 0, nil
    }
    return binary.BigEndian.Uint16(b), nil
}

func writeVersion(ctx context.Context, tx gorp.Tx, key []byte, v uint16) error {
    b := make([]byte, 2)
    binary.BigEndian.PutUint16(b, v)
    return tx.Set(ctx, key, b)
}
```

### Service Wiring

```go
// Example: how a service registers and runs migrations at startup.
//
// core/pkg/service/schematic/migrations/migrate.go
package migrations

import "github.com/synnaxlabs/oracle"

// All returns the ordered migrations for the Schematic type.
func All() []oracle.Migration {
    return []oracle.Migration{
        oracle.RegisterTyped("msgpack_to_protobuf", MigrateV1ToV2),
        // Future migrations appended here:
        // oracle.RegisterTyped("add_description", MigrateV2ToV3),
        // oracle.RegisterRaw("restructure_symbols", MigrateV3ToV4),
    }
}

// core/pkg/service/schematic/service.go
func OpenService(ctx context.Context, cfg ServiceConfig) (*Service, error) {
    // Run key-encoding migrations (existing EntryManager behavior).
    entryManager, err := gorp.OpenEntryManager[uuid.UUID, Schematic](ctx, cfg.DB)
    if err != nil {
        return nil, err
    }

    // Run Oracle schema migrations.
    migrator := oracle.Migrator[uuid.UUID, Schematic]{
        Migrations: migrations.All(),
    }
    if err := migrator.Run(ctx, cfg.DB); err != nil {
        return nil, err
    }

    // ... rest of service init ...
}
```

### Migration File Layout (Concrete Example)

```
core/pkg/service/schematic/
├── types.gen.go              # Current Schematic type (Oracle-generated)
├── service.go                # Service init, calls migrator.Run()
└── migrations/
    ├── migrate.go            # All() function returning ordered migrations
    ├── v1.go                 # Oracle-generated: SchematicV1 type + transform
    └── v1_test.go            # Tests using built-in helpers
```

**v1.go** (fully Oracle-generated for codec transition):

```go
package migrations

import (
    "context"

    "github.com/google/uuid"
    "github.com/synnaxlabs/x/binary"
    "github.com/synnaxlabs/oracle"
    "github.com/synnaxlabs/synnax/core/pkg/service/schematic"
)

// SchematicV1 is the schema snapshot at version 1 (msgpack encoding).
type SchematicV1 struct {
    Key      uuid.UUID                 `json:"key" msgpack:"key"`
    Name     string                    `json:"name" msgpack:"name"`
    Data     binary.MsgpackEncodedJSON `json:"data" msgpack:"data"`
    Snapshot bool                      `json:"snapshot" msgpack:"snapshot"`
}

// MigrateV1ToV2 transforms SchematicV1 to the current Schematic type.
func MigrateV1ToV2(ctx context.Context, old SchematicV1) (schematic.Schematic, error) {
    return schematic.Schematic{
        Key:      old.Key,
        Name:     old.Name,
        Data:     old.Data,
        Snapshot: old.Snapshot,
    }, nil
}
```

### Multi-Step Chaining Design

When multiple migrations are pending (e.g., v1→v2→v3), the current design runs each
migration as a full pass over all entries. This means:

- **v1→v2**: Iterate all entries, decode as V1, transform to V2, encode, write.
- **v2→v3**: Iterate all entries again, decode as V2, transform to V3, encode, write.

This is simple but involves multiple full passes. An optimization would be to chain
transforms in memory (decode once, apply all transforms, encode once, write once).
However, this complicates the runner because:

1. Intermediate types exist only at compile time — the runner can't dynamically chain
   `func(V1) V2` and `func(V2) V3` without generics gymnastics.
2. Raw migrations can't be chained — they need the KV state from the previous step.

**Decision for now**: Multiple full passes. Each migration is a complete read-transform-
write cycle. Simple, correct, and fast enough for metadata volumes (thousands of
entries, not millions). Optimize later if startup time becomes a concern.

### Codec Transition: Old vs New Codec

For the msgpack→protobuf MVP, the `runConfig` needs different codecs for reading and
writing:

```go
cfg := runConfig{
    prefix:   prefix,
    oldCodec: &binary.MsgPackCodec{},   // decode existing msgpack data
    newCodec: &binary.ProtobufCodec{},  // encode as protobuf
}
```

This requires the migration to declare which codecs it uses. Options:

1. **Migration-level codec override**: Each `Migration` optionally specifies old/new
   codecs. The runner uses the DB's default codec if not specified.
2. **Global codec switch**: The DB's codec changes from msgpack to protobuf, and the
   first migration for each type handles the transition using a `decodeFallbackCodec`
   (which already exists in `binary.NewDecodeFallbackCodec`).

Option 2 is simpler: change the DB codec to protobuf globally, and use
`NewDecodeFallbackCodec(protobuf, msgpack)` so reading works with both old and new
formats. The first write to any entry re-encodes it as protobuf. Combined with the
eager migration pass (which reads and writes every entry), all entries get re-encoded.

```go
// In distribution layer setup:
codec := binary.NewDecodeFallbackCodec(
    &binary.ProtobufCodec{},  // try protobuf first
    &binary.MsgPackCodec{},   // fall back to msgpack
)
db := gorp.Wrap(kvStore, gorp.WithCodec(codec))
```

This means the codec transition doesn't even need per-type migration files — the
fallback codec + eager re-encode pass (which EntryManager already does via
`reEncodeKeys`) handles it automatically. **However**, this conflicts with Decision 3
(all changes require a migration entry). The migration entry for the codec transition
could be a no-op that exists purely for the audit trail.

This is a design tension that needs resolution.

## Open Questions / Trade Studies

### Codec Transition Approach (from Scenario 1)

**Decision**: Oracle-generated per-type migration files.

15 individual migration files is fine as long as Oracle generates them fully — the
critical principle is minimizing user-written code. For purely mechanical migrations
(codec transitions, trivial field copies), Oracle generates the complete transform
function, not just a skeleton. Decision 19 (always empty skeleton) applies to
schema-change migrations where the developer must make judgment calls. Codec transitions
are deterministic and can be fully auto-generated.

This means Oracle has two generation modes:

- **Skeleton mode**: For schema changes (field add/remove/rename/type change). Generates
  legacy type + empty transform. Developer fills in logic.
- **Full mode**: For mechanical migrations (codec transitions). Generates legacy type +
  complete transform. No developer input needed.

### Dependency Injection for Raw Transaction Migrations (from Scenarios 7, 8)

**Decision**: Enrich the raw transaction escape hatch with injectable dependencies.

The migration function signature for raw transaction migrations will accept additional
injected dependencies beyond `gorp.Tx`. The exact injection mechanism (context object,
closure capture, or service method) is an implementation detail to resolve during API
surface design. The key requirement: cross-type migrations must be able to access
ontology writers, group services, user services, and other service-layer dependencies.

### Unknown JSON Keys During Strong-Typing (from Scenario 5)

When promoting a `json` field to a struct, stored entries may contain keys the struct
doesn't define (client-written extra fields, deprecated fields). Options:

- **Drop unknown keys**: Only migrate known fields. Data loss but clean types.
- **Catch-all field**: Add a `map[string]any` field for overflow. Preserves data but
  adds complexity.
- **Error on unknown**: Migration fails if unexpected keys found. Strict but may break
  on real data.

### Schema Snapshot Format (Decision 14)

How should Oracle store schema state for CI change detection and legacy type generation?
Options: committed `.oracle` snapshots, structured AST, or hash + generated Go code.

### Non-Oracle Type Migration (Decision 20)

Should non-Oracle gorp-stored types (Cesium, Aspen, ontology) be converted to Oracle
schemas, supported alongside Oracle types in the migration system, or keep their own
mechanisms?

## Architecture Overview

```
Developer Workflow:
  1. Modify .oracle schema file
  2. Run `oracle migrate generate`
     → Oracle snapshots old type into service/migrations/vN.go
     → Oracle generates empty transform function skeleton
     → Oracle updates schema snapshot for CI
  3. Developer writes transform logic in vN.go
  4. Run tests using built-in migration test helpers
  5. Commit — CI validates schema snapshot matches migrations

Server Startup:
  1. Each service opens its EntryManager
  2. Migration runner checks __oracle__/<type>/version in KV store
  3. For each pending migration (currentVersion+1 → latest):
     a. Iterate all entries of the type
     b. Decode each entry using legacy type
     c. Call typed transform function: old → new
     d. Write back entry using current type
     e. Increment version counter
  4. Service begins accepting requests

Directory Layout:
  core/pkg/service/schematic/
  ├── schematic.go           # Current type (Oracle-generated)
  ├── types.gen.go           # Generated Go code
  ├── service.go             # Service logic
  └── migrations/
      ├── v1.go              # SchematicV1 type + v1→v2 transform
      ├── v2.go              # SchematicV2 type + v2→v3 transform
      └── migrate.go         # Migration registration
```

## Roadmap

1. **Phase 1 (This RFC)**: Oracle migration framework + msgpack→protobuf codec switch
2. **Phase 2**: Incrementally strongly type JSON fields (schematic data, workspace
   layout) using the migration system
3. **Phase 3**: RFC 0025 — server owns all schema evolution, replacing client-side
   migrations

## Decision Summary

| # | Topic | Decision |
|---|-------|----------|
| 1 | Version granularity | Per-type with dependency tracking |
| 2 | Change detection | Manual generation + CI enforcement |
| 3 | Migration-worthy changes | All changes require migration |
| 4 | Function signature | Typed per-entry transform + raw tx escape hatch |
| 5 | Execution strategy | Eager on server startup |
| 6 | Upgrade path | Sequential chaining, forward-only, no rollback |
| 7 | Directory layout | Per-service `migrations/` directory |
| 8 | Legacy type generation | Auto-generated snapshots by Oracle |
| 9 | Protobuf scope | Replace msgpack entirely |
| 10 | Codec migration | Oracle-generated |
| 11 | JSON field typing | Full structs, adopted incrementally |
| 12 | Source of truth | Gradual replacement, Oracle eventual sole source |
| 13 | Multi-node coordination | Each node migrates independently |
| 14 | Schema snapshot format | **TBD — trade study needed** |
| 15 | Proto field numbers | Not a concern (clients always in sync) |
| 16 | Version counter storage | Single KV key per type |
| 17 | Version counter size | uint16 (65,535 max) |
| 18 | Existing infrastructure | Oracle replaces gorp.Migrator |
| 19 | Stub generation | Always empty skeleton |
| 20 | Non-Oracle types | **TBD — trade study needed** |
| 21 | MVP / first use case | msgpack → protobuf codec transition |
| 22 | Testing | Built-in test helpers |
| 23 | Relationship to RFC 0025 | Foundation that 0025 builds on |
