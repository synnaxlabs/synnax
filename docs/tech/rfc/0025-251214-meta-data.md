# 25 - Meta Data Structures

**Feature Name**: Improvements to Meta-Data Structure Handling

# 0 - Summary

In this RFC I propose a set of standard practices, patterns, and tools for working with
meta-data structures in Synnax. Meta-data structures are any permanently stored
resources with a Synnax deployment that are not telemetry. This generally includes
any data structure stored in Pebble/Aspen as opposed to Cesium: think Users, Access
Control Policies, Schematics, Logs, Tables, etc

# 1 - Motivation

Although generally considered somewhat secondary in importance and complexity when
compared to Synnax's core telemetry engine, the significant increase in functionality,
logic, and code volume related to meta-data structures in the Synnax codebase means
they form a critical part of every user workflow.

Although some patterns (gorp, services, clients, flux queries, slices) have been
introduced to standardize the toolchains for working with these data structures, the high
variability in their presentation and core logic has resulted in a number of poor
practices that are now significantly hindering the maintainability and stability
of the Synnax platform.

## 1.0 - Toolchain for a Meta-Data Structure

A "toolchain" for a data structure represents the set of services
(in the core, client libraries, driver, pluto, and console) used to support the feature
set that data structure aims to implement.

For examples, the toolchain for a range includes:

1. The service in the core used to create, retrieve, and validate ranges (core/service).
2. Migrations for updating data structures on existing deployments.
3. Mechanisms for serializing, storing, and indexing ranges on disk (gorp, aspen, pebble).
4. Signal propagation mechanisms for real-time synchronization and collaborative editing (distribution/signals).
5. API endpoints and access control mechanisms for those endpoints (core/api).
6. Transport mechanisms for encoding and transmitting data structures over the network (freighter).
7. Client libraries for adding language-native wrappers around API calls,
   and exposing standardized data structures (client/ts, client/py, client/cpp)
8. Reactive queries for updating UI's in real-time (flux)
9. General purpose components for exposing functionality (pluto).
10. Specific UI views for user interaction (console).

Toolchains vary across structures, although a large portion of their components clearly
overlap.

## 1.1 - Problems with Current Toolchains

### 1.1.0 - Lack of Efficient Query Mechanisms for Meta-Data Structures

The initial implementation of gorp was designed around simplicity and minimum
viability: *create a wrapper around a key-value store to provide access and rudimentary
query mechanisms for go data structures*.

Gorp's implementation has evolved very little over the course of several years in
production, while the complexity and performance requirements of the services that
use it have evolved significantly.

Gorp uses inefficient `msgpack` or `json` encoding/decoding mechanisms that leverage
heavy reflection, chewing through heap allocations and cpu instruction time. We have
access to a performant, optimized cache for meta-data inside of Pebble, but we can't
leverage it because of the overhead of serialization and/or deserialization.

Gorp supports fast lookups of data structures by key, but that's about it. Nowadays,
we're doing large numbers of lookups by filtering through fields (name, data_type, etc.).
For any of these non-key based lookups, gorp iterates through *every single* item for
a particular class of data structure.

For example, if you want to look up a range by its name, gorp will iterate through
*every* range stored in the database.

We also have limited support for ordering operations, particularly when the ordering
is combined with additional filters.

### 1.1.1 - Large Amounts of Boilerplate

A significant proportion of the Synnax codebase is replicated boilerplate allocated
to supporting services. Although patterns are generally clear and well established,
maintaining this boilerplate has a non-negligible overhead.

Almost every data structure has a similar shape to its core service, API, and client
library implementations (x3). It's hard to believe there's not some way to consolidate
and simplify the process of maintaining these 'boilerplatey' services without resulting
in excessive coupling.

### 1.1.2 - Lack of Server Side Undo/Redo Support, and Limited Versioning Support for Data Structures

One of the hallmarks of a good user experience is providing leniency when the user makes
a mistake. There comes a huge amount of pain from accidentally deleting something
that is unrecoverable, and a fair amount of joy that comes when the `Ctrl+Z` button
works as expected.

Our leniency for user mistakes is remarkably poor. Very few of our data structures have
undo/redo support, and the support we do have is not very good. Messing something
up in Synnax usually has a high consequence.

### 1.1.3 - Client Side Migrations for Server Side Data Structures

The source of truth for many data structures is in the client side, even though they
are stored on the server side. The most notable are visualizations (line plots, logs,
tables, etc.) and tasks. Migrations are applied on the client side, most commonly when
booting up the Console.

This pattern introduces a number of problems:

1. **Migration Code is Brittle**: Migrations require this delicate dance of synchronizing
state between the server, flux queries, and redux store. We commonly have to have a number
of different version checks, strange synchronization mechanisms, and other error-prone
code in order to properly keep migrations in sync across data storage locations.
2. **Incompatibility with C++, Python SDKs, and APIs**: As migrations are executed from
the console, any client libraries that would like to programmatically work with these
data structures must either tolerate unpredictable versions OR wait for the console
to magically migrate data structures the next time a user accesses it.

### 1.1.4 - Inefficient Network Transport for Synchronization of Complex Data Structures

Some of our more complex data structures in the console leverage partial state updates
in ordered to selectively mutate specific properties or fields. Tasks and ranges use
the form API and visualizations such as schematics and line plots use reducers.

In all of these examples, even minor changes result in the **entire** data structure
getting sent to the core. For example, panning a schematic mutates a maximum of two
properties, but we send the entire schematic, which can have thousands of properties,
to the core.

Adding support for real-time collaboration is also impossible, as sending entire
schematics makes it difficult to merge changes across concurrent user editing.

### 1.1.5 - Limited Support for Collaborative Editing

The limitations of synchronizing entire data structures also makes real-time
collaboration challenging, as efficiently identifying, merging, and communicating
concurrent changes to state

### 1.1.6 - Data Structure Complexity Due to Negligence and Technical Debt

Many of our data structures have duplicated fields with different names, badly managed
properties, and lack standardized conventions across the codebase.

Our Schematics are, once again, notable examples of this. The `Edge` type in the diagram
has both `key` and an `id` field. The `Edge` type also has a `data` field that is
optionally populated.

This lack of discipline when defining data structures has resulted in a number of critical bugs.

### 1.1.7 - Over Reliance on Runtime Validation + General Lack of Type Safety

For a number of data structures such as tasks, devices, and visualizations, we store
strings on the backend. Doing so provides a high degree of flexibility for introducing
additional variants of these structures, but it also means that we're sacrificing a
significant amount of compile-time safety.

### 1.1.8 - Multiple Sources of Truth in Console-Side State

There are two primary locations where we store data structures in global state: Flux
and Redux. Not all data structures are stored in Redux, but for the ones that are,
synchronizing state between flux queries and the global redux store is a major challenge.
Doing so typically requires effect heavy code that introduces unpredictable delays and
sudden synchronization bugs. Notable example include:

1. **Ranges**: Keeping the range toolbar synchronized with ranges store in flux.
2. **Visualizations**: Delayed synchronization leads to `undefined` errors being
thrown in the toolbar. Complex `useLoadRemote` and `useSyncDispatch` are patches.

### 1.1.9 - Non-Standard Management of Relationships Between Data Structures

There hasn't been any established rule for defining, managing, and querying relationships
between data structures. A few of the following patterns have been used or are still
being used:

1. **Ontology Relationships**: Good because they provide a high degree of flexibility
and a clear separation between data structures and the tooling defining their
relationships. Bad because they allow pretty much anything to be related to anything,
the performance is generally not that great.
2. **Explicit Fields**: The most performant and explicit, but are quite inflexible,
and require strong dependencies for managing things like deletion waterfalls.
3. **Keys**": Certain explicit parent to child relationships have their parent resource
as part of their key. Examples include range aliases and range key-value pairs. More
explicit than ontology relationships, but also add a lot of brittleness.

### 1.1.10 - Data Structure Replication through Ontology

The ontology's ability to retrieve resources along with their fields is both a blessing
and a curse. It allows for efficient display of non-homogeneous items in places like
our ontology toolbars and makes it easy to add general purpose search indexes,
but it also introduces key problems:

1. **More Code to Maintain**: We now need to maintain `zyn` schemas, complex
`ontology.Service` implementations with lots of boilerplate, and the ontology retrieval
code itself takes a performance hit.
2. **Synchronization**: Right now every data structure can have potentially two copies
in `flux` state. One in the ontology resource store and one in the store for the
data structure itself. This means we need to write flux code to synchronize everything,
adding complexity.

### 1.1.11 - Poor Behavior of Queries that Mix Filtering, Sorting, and Pagination

Our query mechanisms for meta-data structures are quite primitive. We run into a number
of issues when we attempt to handle a mix of filtering, sorting, and pagination
operations.

#### 1.1.11.0 - Ephemeral vs. Persisted State

In Views, for example, we have to pieces to a query: it's persisted vs. it's ephemeral
state. Persisted state includes filtering operations (i.e. `ranges with these labels`)
and sorting operations (i.e. `sort ranges by time range`) while ephemeral state represents
pagination (i.e. `this offset + this limit`).

Right now, we have bad management of the process of 'resetting ephemeral state'. When
do we reset the pagination parameters? This seems simple in practice, but we have a ton
of mangled TypeScript to manage the current query state.

##### 1.1.11.1 - Poor Pagination Mechanics

We do all of our sorting client side. This means that we can't provide any form of
stable ordering when we apply sorting rules and have multiple pages. We also do a very
bad job of keeping track of the data available in pagination. We rely on the client
side to do a 'pseudo-detection' of the last page by simply checking if the amount of
returned data is equal to the limit. If it's not, then there must be no more data left.

###### 1.1.11.2 - Unstable Caching of Queries

We have the ability to do primitive caching of list queries through flux. This is
awesome because it reduces the time to interaction for many components. The problem
is that whenever we do a remote fetch to replace stale data, we end up with this weird
visual 'jumping' that replaces data in the list. This is jarring for the user.

We don't have ways of maintaining the stability of queries across cached results and
the remote.

### 1.1.12 - Lack of Standardization for Persisted vs. User Side State

The vast majority of our visualizations require a distinction between the persisted
configuration of the visualization and the current user's view of it. Examples include:

1. The lines plotted on a line plot (configuration) vs. the current viewport (user state).
2. The type for a particular channel in a task (configuration) vs. the currently selected
channel (user state).

The problem is that several of our current data structure definitions mangle this state.
We have our definition of the current user `viewport` as a field right next to the set
of `lines` that belong in a plot. These are all saved to the core together, even though
we don't really want `viewport` to be saved as part of the configuration.

# 2 - Improving the Toolchain

## 2.1 - Sorting, Filtering, Pagination Improvements

Sorting, filtering, and pagination are some of the most poorly implemented pieces
of meta-data functionality in Synnax.



## 2.2 - Faster, More Efficient Query Engine

## 2.3 - More Efficient Encoding Mechanisms

## 2.4 - Standardized Type Generation

Core types across languages should be generated from a single source of truth.

### 2.5 - Core-Side Migrations for All Primary Data Structures

Any data structure that is stored in the core should be migrated on the core.

### 2.6 - No More Stringified JSON Configurations

Lots of dynamic typing for things is pretty bad.

### 2.7 - Single Source of Truth in Client-Side State

### 2.8 - Consider Removing Retrieval of Ontology Data

### 2.9 - Partial Update Tooling

### 2.10 - Client Side Queues and Transaction Mechanics

### 2.11 - Support for Core Side Undo/Redo

## Research on Migrations

Generally speaking, there are two classes of migration engines:

1. Model as truth - The current state of the data structure is the source of truth.
   Migrations are created via code generation in order to evolve the model from its first
   structure to the current structure. This is more declarative. Data shape yields migrations.
2. Migrations as truth - An imperative approach, where the chain of functions that perform
   the migration are the source of truth. Migrations yield data shape.

# 3 - Implementation Order
