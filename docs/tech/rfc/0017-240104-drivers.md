# 16 - General Purpose Device Drivers

**Feature Name**: General Purpose Device Drivers <br />
**Start Date**: 2024-01-04 <br />
**Authors**: Emiliano Bonilla <br />
**Status**: Draft <br />

# 5 - Detailed Design

## 5.0 - Overview

The highest level of the device driver is called a rack. A rack manages a set of tasks,
which are in charge of acquiring data or executing commands on devices connected to
the computer the driver is running on. Tasks are completely independent of one another,
are only allowed to communicate with a single device, and can only perform data
acquisition or control, not both. The rack configures tasks based on a provided
specification, starts and stops tasks, and deletes them when they are no longer needed.

## 5.1 - The Rack

A rack represents a set of data acquisition and control tasks running within a single
program. For example, a running instance of the driver would represent a single rack in
the cluster. Another example of a rack could be a custom flight computer that communicates
directly with Synnax over the network.

## 5.2 - Representation Inside the Synnax Server

A rack persists some of its meta-data inside the Synnax Server for access by other parts
of the platform, such as the UI. Here are the fields we're going to store inside of
Synnax:

```go
package irrelivant

type Rack struct {
    Key uint32
    Name string
}
```

The `Key` field would represent two composite `uint16s` where the first `uint16` is the
**leaseholder** of the rack i.e. which node the rack writes all of its data to. A rack
can write or read data from channels on any node, but must use this node as its gateway
for all communications. The second uint16 is simply an incrementing counter identifying
a particular rack within the scope of its leaseholder. For example, a rack could be
identified as `Node 5 Rack 2` or `Node 7 Rack 8`. Limiting the number of racks per node
to 65535 is a little bit risky, but it helps make the key much more compact.

### 5.3 - Rack Responsibilities

A rack has three responsibilities:

1. Discovering new devices and informing Synnax server of their existence.
2. Emitting a heartbeat at a consistent interval to communicate its health.
3. Configuring and commanding the acquisition and command tasks it's responsible for.

### 5.4.1 - Heartbeat

A rack emits a heartbeat over the `sy_rack_heartbeat` channel at a pre-determined rate
(we're setting this at 1 second to start). The heartbeat is 64-bit number, where the
first 32 bits are the key of the rack, and the second 32 bits are the version, a number
that is incremented every time the rack emits a heartbeat. Naturally, this version
resets to zero every time the rack restarts.

From an implementation perspective, the heartbeat mechanism will run on a separate
thread.

### 5.4.2 - Task Configuration

On bootup, the rack starts listening on `sy_task_set` and `sy_task_delete` for task
creation, configuration, and deletion operations. Internally, the rack maintains
a map of task keys to their corresponding tasks. When a task is created or configured,
the rack looks for a task in the map with the same key. If one exists, that task is
stopped and removed from the map. The rack then re-configures the task, starts it, and
sets it back on the map. It's important to note that re-configuring a task involves
the re-allocation and re-construction of an entirely new data structure inside the driver.
This process is marginally more expensive than calling an `update` method with a new
configuration, but removes the complexity of needing to manage concurrent updates while
a task is running.

Task configuration is a relatively infrequent (once every few seconds, at most) operation,
so it's totally fine to make the process more expensive for the sake of keeping the system
simple.

### 5.4.3 - Adjusting Task State

## 5.0 - Command and Acquisition Pipelines

Driver functionality can be separated into two distinct pipelines: acquisition and command.
Acquisition pipelines acquire data from sensor hardware and forward it to Synnax, while
command pipelines receive commands from Synnax and execute them on the hardware. This
marks a clear point of design separation, and has a few beneficial properties:

1. Command and acquisition pipelines don't need to share state (aside from what is
   implemented in hardware specific libraries like DAQmx).
2. The failure of a command pipeline should not affect the operation of an acquisition
   pipeline, and vice versa.

These two properties reduce the cognitive overhead, as both pipelines can be implemented
independently, only sharing essential primitives. Naturally, each pipeline should be
executed in its own thread.

## 5.1 - Acquisition Pipeline Overview

Acquisition pipelines operate as a continuous acquisition loop that performs three tasks:

1. Acquire data from hardware.
2. Apply any necessary calibrations, transformations, taring.
3. Write data to Synnax.

This loop should run at a nearly-fixed rate, and consume a well-defined amount of CPU,
memory, and DAQ device resources (we'll touch more on timing later). From a high level,
implementing this loop seems quite simple. There are three emergent details that make it
far more difficult:

1. **Error handling** - Happy path execution is simple, but errors can occur in any of
   these steps. Some of these errors are critical and should halt pipeline execution,
   while some are transient and require retries or pipeline restarts. Correctly
   identifying, communicating, and handling these errors requires careful design.

2. **Configuration** - Every pipeline requires detailed configuration information to
   operate: channel names, physical device ports, calibrations, etc. These
   configurations are dynamic, and need to be updated mid-driver operation. Invalid
   configuration parameters are an additional source of errors, and need to be carefully
   communicated to and resolved by the user.

3. **Hardware 'Polymorphism'** - We're aiming to support DAQ hardware from a growing
   number of vendors, and, for the most part, the loop structure remains the same no
   matter the device. Ideally we'd make it possible to keep the majority of the pipeline
   code the same, and develop a standard interface for implementing step #1.

## 5.2 - Command Pipeline Overview

## 5.3 - Configuration

There are important properties and patterns to examine.

### 5.3.0 - Properties

#### 5.3.0.0 - Dynamism

Adaptive teams are always making changes to their system configuration: adding and
exchanging sensors, changing calibrations, and swapping DAQ tasks. From a user
perspective, these changes are quite frequent. During setup batched configuration
changes can come every few minutes. From a software, perspective, however, these changes
are _very_ infrequent. If a pipeline runs at 200hz and a configuration change comes
every ten minutes, that's 120,000 pipeline iterations between each application. Even
using 100 pipeline iterations (0.5s) to fetch and apply configuration changes would be
almost negligible to operation.

In consequence, it's worthwhile to make the configuration process more expensive to
achieve the following:

1. Improve the configuration process for the user by providing clear, reliable feedback.
2. Improve hot path (i.e. pipeline loop) performance and reduce complexity.

#### 5.3.0.1 - Flexibility

Different organizations configure different hardware in different ways. This makes it
largely impossible to define a fixed schema for configuration parameters; attempting to
do so would not only bloat the codebase, but result in very tight coupling between the
driver, core, and frontend.

#### 5.3.0.2 - Error Variants and Emergence

There are two levels of errors that can occur during configuration: critical and warning.
Critical errors mean the system can't operate, while warnings allow the system to run
but still must be communicated.

The more complex aspect of error handling is that configuration errors can occur at two
points: during pipeline setup and during pipeline execution. For example, the hardware
may validate the acquisition rate, but during runtime an ADC may not be able to keep
pace with the system.

### 5.4 - The Rack

#### 5.4.0 - Overview




### 5.5 - The Task

A task is an independent acquisition or control loop within a rack. Each task
communicates with a single device from a single vendor, and operates a command or
acquisition pipeline. The pipelines of a task have a consistent acquisition and
command acknowledgement rate. For example, a task could represent a continuous
100Hz acquisition process from a National Instruments cDAQ.

By guaranteeing a single sample rate, a single device, and a single vendor, we can
considerably reduce the complexity ceiling of a task. We won't need to write code that
synchronizes timing or configuration between multiple devices from different vendors,
and we can complete all necessary tasks of a task with a single or small number of
threads.

Pseudocode for the data structure is as follows:

```go
package irrelivant

type Task struct {
   Key    uint64
   Name   string
   Type   string
   Config string
}

```

The `Key` field is two composite `uint32s`, where the first `uint32` is the key of a
task's rack, and the second is an incremented counter identifying a rack within
its task. As with a Rack, a task could be identified as `Node 5 Rack 2 Task 5`.

The`Type` field enables different task implementations to operate within the same
rack, and serves as the identifier fora driver side abstract factory to select and
configure the type's task class. The same pattern can be applied for any
task specific interfaces on the frontend.

Config is a JSON encoded string storing the configuration for that type of task. While
it reduces the amount of server side validation that can be completed on a task, it
does enable us to expand the number of DAQ vendors and add custom hardware without
needing to change Synnax Core.

We've debated about whether we should include the `config` field directly on the task
struct or not. Configuration info probably won't be very large (a few hundred kilobytes
at most), and it will be rare to query or list more tasks than a rack contains (which
is almost always less than 10). This is a two-way decision whose interface does not
affect the end user, so we can always move the config to its own struct if necessary.

## Working Notes

### How do we communicate task configuration and runtime errors?

1. Rack is in charge of starting, stopping, and
